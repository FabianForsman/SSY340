{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee0f6b2e",
   "metadata": {},
   "source": [
    "# Hate Speech Detection - Data Exploration\n",
    "\n",
    "This notebook explores the Hate Speech and Offensive Language Dataset.\n",
    "\n",
    "**Project:** SSY340 - Unsupervised Learning for Hate Speech Detection  \n",
    "**Group:** 13  \n",
    "**Dataset:** Hate Speech and Offensive Language Dataset (~25,000 tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c13206",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from data_loader import DataLoader\n",
    "from preprocessing import TextPreprocessor, get_text_stats\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"Imports complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d210da57",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca79797f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "loader = DataLoader('../data/raw')\n",
    "df = loader.load_hate_speech_dataset()\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738b00f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataset information\n",
    "loader.get_dataset_info(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5d6a21",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b0470f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "df['class'].value_counts().plot(kind='bar')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Label Distribution')\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "df['class'].value_counts().plot(kind='pie', autopct='%1.1f%%')\n",
    "plt.ylabel('')\n",
    "plt.title('Label Distribution (%)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nClass distribution:\")\n",
    "print(df['class'].value_counts())\n",
    "print(f\"\\nClass proportions:\")\n",
    "print(df['class'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c30877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text length analysis\n",
    "text_column = 'tweet'  # Change if different\n",
    "\n",
    "df['text_length'] = df[text_column].str.len()\n",
    "df['word_count'] = df[text_column].str.split().str.len()\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Character length distribution\n",
    "axes[0, 0].hist(df['text_length'], bins=50, edgecolor='black')\n",
    "axes[0, 0].set_xlabel('Character Length')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('Distribution of Text Length')\n",
    "\n",
    "# Word count distribution\n",
    "axes[0, 1].hist(df['word_count'], bins=50, edgecolor='black', color='orange')\n",
    "axes[0, 1].set_xlabel('Word Count')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title('Distribution of Word Count')\n",
    "\n",
    "# Text length by class\n",
    "for class_label in df['class'].unique():\n",
    "    data = df[df['class'] == class_label]['text_length']\n",
    "    axes[1, 0].hist(data, bins=30, alpha=0.5, label=f'Class {class_label}')\n",
    "axes[1, 0].set_xlabel('Character Length')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].set_title('Text Length by Class')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Box plot of text length by class\n",
    "df.boxplot(column='text_length', by='class', ax=axes[1, 1])\n",
    "axes[1, 1].set_xlabel('Class')\n",
    "axes[1, 1].set_ylabel('Character Length')\n",
    "axes[1, 1].set_title('Text Length Distribution by Class')\n",
    "\n",
    "plt.suptitle('')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nText statistics:\")\n",
    "print(df[['text_length', 'word_count']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc53b323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample tweets from each class\n",
    "print(\"Sample tweets from each class:\\n\")\n",
    "for class_label in sorted(df['class'].unique()):\n",
    "    print(f\"\\n=== Class {class_label} ===\")\n",
    "    samples = df[df['class'] == class_label].sample(3, random_state=42)\n",
    "    for idx, tweet in enumerate(samples[text_column].values, 1):\n",
    "        print(f\"{idx}. {tweet}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c2f9af",
   "metadata": {},
   "source": [
    "## 3. Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5ccdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessor\n",
    "preprocessor = TextPreprocessor(\n",
    "    remove_stopwords=False,\n",
    "    lowercase=True,\n",
    "    remove_urls=True,\n",
    "    remove_mentions=True,\n",
    "    remove_hashtags=False\n",
    ")\n",
    "\n",
    "# Preprocess sample tweets\n",
    "sample_tweets = df[text_column].sample(5, random_state=42).tolist()\n",
    "\n",
    "print(\"Preprocessing examples:\\n\")\n",
    "for i, tweet in enumerate(sample_tweets, 1):\n",
    "    cleaned = preprocessor.clean_text(tweet)\n",
    "    print(f\"{i}. Original: {tweet}\")\n",
    "    print(f\"   Cleaned:  {cleaned}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4e4a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess entire dataset\n",
    "df_processed = preprocessor.preprocess_dataframe(df, text_column, 'cleaned_text')\n",
    "\n",
    "# Compare statistics before and after\n",
    "print(\"Before preprocessing:\")\n",
    "stats_before = get_text_stats(df[text_column])\n",
    "for k, v in stats_before.items():\n",
    "    print(f\"  {k}: {v:.2f}\")\n",
    "\n",
    "print(\"\\nAfter preprocessing:\")\n",
    "stats_after = get_text_stats(df_processed['cleaned_text'])\n",
    "for k, v in stats_after.items():\n",
    "    print(f\"  {k}: {v:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57373aea",
   "metadata": {},
   "source": [
    "## 4. Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c46be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed data\n",
    "output_path = '../data/processed/processed_data.csv'\n",
    "df_processed.to_csv(output_path, index=False)\n",
    "print(f\"Saved processed data to {output_path}\")\n",
    "print(f\"Shape: {df_processed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48dbe9e",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook explored the Hate Speech and Offensive Language Dataset, including:\n",
    "- Dataset structure and label distribution\n",
    "- Text length and word count statistics\n",
    "- Sample tweets from each class\n",
    "- Text preprocessing pipeline\n",
    "\n",
    "Next steps:\n",
    "1. Generate embeddings using SBERT models\n",
    "2. Apply clustering algorithms (K-Means, DBSCAN)\n",
    "3. Evaluate using ARI and other metrics"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
